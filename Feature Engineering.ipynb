{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b9403cb-97da-481a-87c8-fb3d37df649a",
   "metadata": {},
   "source": [
    "# Churn Prediction - Feature Engineering\n",
    "---\n",
    "**Table of Contents** :\n",
    "1. Import packages and Define functions\n",
    "2. Load data\n",
    "3. Feature Engineering\n",
    "    - Create variables that capture price differentials, reflecting customer price sensitivity\n",
    "    - Transform `datetime` features into numeric values representing the number of months (as integers)\n",
    "4. Data Preprocessing\n",
    "   - Encoding Categorical Data\n",
    "   - Addressing Skewness in Numerical Data\n",
    "   - Correlation Analysis\n",
    "5. Export the data for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c828eb73-21d0-4064-8af4-a51fe37b7198",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Import packages and Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdfb1b47-ba06-4350-9020-148d98a1b342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfff25b1-13d3-405e-816a-981e57e63e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "from scipy.stats import skew\n",
    "# Shows plots in jupyter notebook\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Set plot style\n",
    "sns.set(color_codes=True)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8967cd9-43fd-4243-b0fe-28a9a5005d08",
   "metadata": {},
   "source": [
    "#### visualization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29103d0e-9a65-4203-9075-bd93cf86ec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stacked_bars(dataframe, title_, size_=(18, 10), rot_=0, legend_=\"upper right\"):\n",
    "    \"\"\"\n",
    "    Plot stacked bars with annotations\n",
    "    \"\"\"\n",
    "    ax = dataframe.plot(\n",
    "        kind=\"bar\",\n",
    "        stacked=True,\n",
    "        figsize=size_,\n",
    "        rot=rot_,\n",
    "        title=title_\n",
    "    )\n",
    "\n",
    "    # Annotate bars\n",
    "    annotate_stacked_bars(ax, textsize=14)\n",
    "    # Rename legend\n",
    "    plt.legend(dataframe.columns, loc=legend_)\n",
    "    # Labels\n",
    "    plt.ylabel(\"Company base (%)\")\n",
    "    plt.show()\n",
    "\n",
    "def annotate_stacked_bars(ax, bins_, pad=0.99, colour=\"black\", textsize=13):\n",
    "    \"\"\"\n",
    "    Add value annotations to the bars\n",
    "    \"\"\"\n",
    "    #print(len(ax.patches))\n",
    "    # Iterate over the plotted rectanges/bars\n",
    "    for i in range(len(ax.patches)):\n",
    "        \n",
    "\n",
    "        if (i<bins_) :\n",
    "        # Calculate annotation\n",
    "            bar_total_height = (ax.patches[i].get_height() + ax.patches[i+int(bins_)].get_height())\n",
    "        else :\n",
    "            bar_total_height = (ax.patches[i].get_height() + ax.patches[i-int(bins_)].get_height())\n",
    "\n",
    "        \n",
    "        value = (ax.patches[i].get_height()/ bar_total_height) * 100\n",
    "\n",
    "        if (np.isnan(value) or value < 5 or (bar_total_height < 50)):\n",
    "            continue\n",
    "            \n",
    "        value = str(round(value,1)) + '%'\n",
    "        #print(value)\n",
    "        \n",
    "        ax.annotate(\n",
    "            value,\n",
    "            #((p.get_x()+ p.get_width()/2)*pad-0.05, (p.get_y()+p.get_height()/2)*pad),\n",
    "            ((ax.patches[i].get_x()), (ax.patches[i].get_y()+ax.patches[i].get_height()/2)*pad),\n",
    "            color=colour,\n",
    "            size=textsize\n",
    "        )\n",
    "\n",
    "def plot_distribution(dataframe, column, ax, bins_=50):\n",
    "    \"\"\"\n",
    "    Plot variable distirbution in a stacked histogram of churned or retained company\n",
    "    \"\"\"\n",
    "    # Create a temporal dataframe with the data to be plot\n",
    "    temp = pd.DataFrame({\"Retention\": dataframe[dataframe[\"churn\"]==0][column],\n",
    "    \"Churn\":dataframe[dataframe[\"churn\"]==1][column]})\n",
    "\n",
    "    #print(temp)\n",
    "    # Plot the histogram\n",
    "    temp[[\"Retention\", \"Churn\"]].plot(kind='hist', bins=bins_, ax=ax, stacked=True)\n",
    "    # X-axis label\n",
    "    ax.set_xlabel(column)\n",
    "    # Change the x-axis to plain style\n",
    "    ax.ticklabel_format(style='plain', axis='x')\n",
    "    annotate_stacked_bars(ax, bins_ = bins_, textsize=14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675b7aa6-9c90-4119-bc65-aecbabf320c5",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93dddbdd-8bcc-40b8-97cc-d3d1e9ace861",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_df = pd.read_csv('./data/client_data.csv')\n",
    "price_df = pd.read_csv('./data/price_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c16bb27b-1d00-40e6-bbda-c0cf80581d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>channel_sales</th>\n",
       "      <th>cons_12m</th>\n",
       "      <th>cons_gas_12m</th>\n",
       "      <th>cons_last_month</th>\n",
       "      <th>date_activ</th>\n",
       "      <th>date_end</th>\n",
       "      <th>date_modif_prod</th>\n",
       "      <th>date_renewal</th>\n",
       "      <th>forecast_cons_12m</th>\n",
       "      <th>...</th>\n",
       "      <th>has_gas</th>\n",
       "      <th>imp_cons</th>\n",
       "      <th>margin_gross_pow_ele</th>\n",
       "      <th>margin_net_pow_ele</th>\n",
       "      <th>nb_prod_act</th>\n",
       "      <th>net_margin</th>\n",
       "      <th>num_years_antig</th>\n",
       "      <th>origin_up</th>\n",
       "      <th>pow_max</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24011ae4ebbe3035111d65fa7c15bc57</td>\n",
       "      <td>foosdfpfkusacimwkcsosbicdxkicaua</td>\n",
       "      <td>0</td>\n",
       "      <td>54946</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-06-15</td>\n",
       "      <td>2016-06-15</td>\n",
       "      <td>2015-11-01</td>\n",
       "      <td>2015-06-23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>t</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.44</td>\n",
       "      <td>25.44</td>\n",
       "      <td>2</td>\n",
       "      <td>678.99</td>\n",
       "      <td>3</td>\n",
       "      <td>lxidpiddsbxsbosboudacockeimpuepw</td>\n",
       "      <td>43.648</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d29c2c54acc38ff3c0614d0a653813dd</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>4660</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-08-21</td>\n",
       "      <td>2016-08-30</td>\n",
       "      <td>2009-08-21</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>189.95</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.38</td>\n",
       "      <td>16.38</td>\n",
       "      <td>1</td>\n",
       "      <td>18.89</td>\n",
       "      <td>6</td>\n",
       "      <td>kamkkxfxxuwbdslkwifmmcsiusiuosws</td>\n",
       "      <td>13.800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>764c75f661154dac3a6c254cd082ea7d</td>\n",
       "      <td>foosdfpfkusacimwkcsosbicdxkicaua</td>\n",
       "      <td>544</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-04-16</td>\n",
       "      <td>2016-04-16</td>\n",
       "      <td>2010-04-16</td>\n",
       "      <td>2015-04-17</td>\n",
       "      <td>47.96</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>0.00</td>\n",
       "      <td>28.60</td>\n",
       "      <td>28.60</td>\n",
       "      <td>1</td>\n",
       "      <td>6.60</td>\n",
       "      <td>6</td>\n",
       "      <td>kamkkxfxxuwbdslkwifmmcsiusiuosws</td>\n",
       "      <td>13.856</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bba03439a292a1e166f80264c16191cb</td>\n",
       "      <td>lmkebamcaaclubfxadlmueccxoimlema</td>\n",
       "      <td>1584</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-03-30</td>\n",
       "      <td>2016-03-30</td>\n",
       "      <td>2010-03-30</td>\n",
       "      <td>2015-03-31</td>\n",
       "      <td>240.04</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30.22</td>\n",
       "      <td>30.22</td>\n",
       "      <td>1</td>\n",
       "      <td>25.46</td>\n",
       "      <td>6</td>\n",
       "      <td>kamkkxfxxuwbdslkwifmmcsiusiuosws</td>\n",
       "      <td>13.200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>149d57cf92fc41cf94415803a877cb4b</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>4425</td>\n",
       "      <td>0</td>\n",
       "      <td>526</td>\n",
       "      <td>2010-01-13</td>\n",
       "      <td>2016-03-07</td>\n",
       "      <td>2010-01-13</td>\n",
       "      <td>2015-03-09</td>\n",
       "      <td>445.75</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>52.32</td>\n",
       "      <td>44.91</td>\n",
       "      <td>44.91</td>\n",
       "      <td>1</td>\n",
       "      <td>47.98</td>\n",
       "      <td>6</td>\n",
       "      <td>kamkkxfxxuwbdslkwifmmcsiusiuosws</td>\n",
       "      <td>19.800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id                     channel_sales  \\\n",
       "0  24011ae4ebbe3035111d65fa7c15bc57  foosdfpfkusacimwkcsosbicdxkicaua   \n",
       "1  d29c2c54acc38ff3c0614d0a653813dd                           MISSING   \n",
       "2  764c75f661154dac3a6c254cd082ea7d  foosdfpfkusacimwkcsosbicdxkicaua   \n",
       "3  bba03439a292a1e166f80264c16191cb  lmkebamcaaclubfxadlmueccxoimlema   \n",
       "4  149d57cf92fc41cf94415803a877cb4b                           MISSING   \n",
       "\n",
       "   cons_12m  cons_gas_12m  cons_last_month  date_activ    date_end  \\\n",
       "0         0         54946                0  2013-06-15  2016-06-15   \n",
       "1      4660             0                0  2009-08-21  2016-08-30   \n",
       "2       544             0                0  2010-04-16  2016-04-16   \n",
       "3      1584             0                0  2010-03-30  2016-03-30   \n",
       "4      4425             0              526  2010-01-13  2016-03-07   \n",
       "\n",
       "  date_modif_prod date_renewal  forecast_cons_12m  ...  has_gas  imp_cons  \\\n",
       "0      2015-11-01   2015-06-23               0.00  ...        t      0.00   \n",
       "1      2009-08-21   2015-08-31             189.95  ...        f      0.00   \n",
       "2      2010-04-16   2015-04-17              47.96  ...        f      0.00   \n",
       "3      2010-03-30   2015-03-31             240.04  ...        f      0.00   \n",
       "4      2010-01-13   2015-03-09             445.75  ...        f     52.32   \n",
       "\n",
       "   margin_gross_pow_ele  margin_net_pow_ele  nb_prod_act  net_margin  \\\n",
       "0                 25.44               25.44            2      678.99   \n",
       "1                 16.38               16.38            1       18.89   \n",
       "2                 28.60               28.60            1        6.60   \n",
       "3                 30.22               30.22            1       25.46   \n",
       "4                 44.91               44.91            1       47.98   \n",
       "\n",
       "  num_years_antig                         origin_up  pow_max  churn  \n",
       "0               3  lxidpiddsbxsbosboudacockeimpuepw   43.648      1  \n",
       "1               6  kamkkxfxxuwbdslkwifmmcsiusiuosws   13.800      0  \n",
       "2               6  kamkkxfxxuwbdslkwifmmcsiusiuosws   13.856      0  \n",
       "3               6  kamkkxfxxuwbdslkwifmmcsiusiuosws   13.200      0  \n",
       "4               6  kamkkxfxxuwbdslkwifmmcsiusiuosws   19.800      0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d50b1fe-8f38-4b2b-9b0c-fd73a97686c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price_date</th>\n",
       "      <th>price_off_peak_var</th>\n",
       "      <th>price_peak_var</th>\n",
       "      <th>price_mid_peak_var</th>\n",
       "      <th>price_off_peak_fix</th>\n",
       "      <th>price_peak_fix</th>\n",
       "      <th>price_mid_peak_fix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>038af19179925da21a25619c5a24b745</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>0.151367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.266931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>038af19179925da21a25619c5a24b745</td>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>0.151367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.266931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>038af19179925da21a25619c5a24b745</td>\n",
       "      <td>2015-03-01</td>\n",
       "      <td>0.151367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.266931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>038af19179925da21a25619c5a24b745</td>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>0.149626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.266931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>038af19179925da21a25619c5a24b745</td>\n",
       "      <td>2015-05-01</td>\n",
       "      <td>0.149626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.266931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  price_date  price_off_peak_var  \\\n",
       "0  038af19179925da21a25619c5a24b745  2015-01-01            0.151367   \n",
       "1  038af19179925da21a25619c5a24b745  2015-02-01            0.151367   \n",
       "2  038af19179925da21a25619c5a24b745  2015-03-01            0.151367   \n",
       "3  038af19179925da21a25619c5a24b745  2015-04-01            0.149626   \n",
       "4  038af19179925da21a25619c5a24b745  2015-05-01            0.149626   \n",
       "\n",
       "   price_peak_var  price_mid_peak_var  price_off_peak_fix  price_peak_fix  \\\n",
       "0             0.0                 0.0           44.266931             0.0   \n",
       "1             0.0                 0.0           44.266931             0.0   \n",
       "2             0.0                 0.0           44.266931             0.0   \n",
       "3             0.0                 0.0           44.266931             0.0   \n",
       "4             0.0                 0.0           44.266931             0.0   \n",
       "\n",
       "   price_mid_peak_fix  \n",
       "0                 0.0  \n",
       "1                 0.0  \n",
       "2                 0.0  \n",
       "3                 0.0  \n",
       "4                 0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559d5877-3a52-4fac-8b92-494083dc30b4",
   "metadata": {},
   "source": [
    "### Dataset Description\n",
    "\n",
    "**client_data** :\n",
    "- id = client company identifier\n",
    "- channel_sales = code of the sales channel\n",
    "- cons_12m = electricity consumption of the past 12 months\n",
    "- cons_gas_12m = gas consumption of the past 12 months\n",
    "- cons_last_month = electricity consumption of the last month\n",
    "- date_activ = date of activation of the contract\n",
    "- date_end = registered date of the end of the contract\n",
    "- date_modif_prod = date of the last modification of the product\n",
    "- date_renewal = date of the next contract renewal\n",
    "- forecast_cons_12m = forecasted electricity consumption for next 12 months\n",
    "- forecast_cons_year = forecasted electricity consumption for the next calendar year\n",
    "- forecast_discount_energy = forecasted value of current discount\n",
    "- forecast_meter_rent_12m = forecasted bill of meter rental for the next 2 months\n",
    "- forecast_price_energy_off_peak = forecasted energy price for 1st period (off peak)\n",
    "- forecast_price_energy_peak = forecasted energy price for 2nd period (peak)\n",
    "- forecast_price_pow_off_peak = forecasted power price for 1st period (off peak)\n",
    "- has_gas = indicated if client is also a gas client\n",
    "- imp_cons = current paid consumption\n",
    "- margin_gross_pow_ele = gross margin on power subscription\n",
    "- margin_net_pow_ele = net margin on power subscription\n",
    "- nb_prod_act = number of active products and services\n",
    "- net_margin = total net margin\n",
    "- num_years_antig = antiquity of the client (in number of years)\n",
    "- origin_up = code of the electricity campaign the customer first subscribed to\n",
    "- pow_max = subscribed power\n",
    "- churn = has the client churned over the next 3 months\n",
    "\n",
    "\n",
    "**price_data**:\n",
    "- id = client company identifier\n",
    "- price_date = reference date\n",
    "- price_off_peak_var = price of energy for the 1st period (off peak)\n",
    "- price_peak_var = price of energy for the 2nd period (peak)\n",
    "- price_mid_peak_var = price of energy for the 3rd period (mid peak)\n",
    "- price_off_peak_fix = price of power for the 1st period (off peak)\n",
    "- price_peak_fix = price of power for the 2nd period (peak)\n",
    "- price_mid_peak_fix = price of power for the 3rd period (mid peak)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40691675-4ba6-406d-9eff-d1a94f313e0e",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Feature Engineering\n",
    "\n",
    "### Annual Price Rate Differentials (January vs December)\n",
    "> **New Features**: Changes in Time-of-Use Rates\n",
    "> - Peak Rate Delta\n",
    "> - Mid-Peak Rate Delta\n",
    "> - Off-Peak Rate Delta\n",
    ">\n",
    "> **Hypothesis**: Significant annual price changes between January and December may impact customer churn decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c116b8ef-b94d-4c16-9cbd-de3c5dcdb022",
   "metadata": {},
   "source": [
    "**Data Preprocessing: Converting Date Fields to Datetime Format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22e1b197-b0eb-4fe1-8a69-e366d434f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_df[\"date_activ\"] = pd.to_datetime(client_df[\"date_activ\"], format='%Y-%m-%d')\n",
    "client_df[\"date_end\"] = pd.to_datetime(client_df[\"date_end\"], format='%Y-%m-%d')\n",
    "client_df[\"date_modif_prod\"] = pd.to_datetime(client_df[\"date_modif_prod\"], format='%Y-%m-%d')\n",
    "client_df[\"date_renewal\"] = pd.to_datetime(client_df[\"date_renewal\"], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62f4c921-958e-42e2-9355-3a2a3b2d30bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'price_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m price_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprice_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m price_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_date\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(price_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_date\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m price_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ml/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ml/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ml/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ml/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ml/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'price_data.csv'"
     ]
    }
   ],
   "source": [
    "price_df = pd.read_csv('price_data.csv')\n",
    "price_df[\"price_date\"] = pd.to_datetime(price_df[\"price_date\"], format='%Y-%m-%d')\n",
    "price_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e58021-068b-4670-9471-69461d80bd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group off-peak prices by companies and month\n",
    "monthly_price_by_id = price_df.groupby(['id', 'price_date']).agg({'price_off_peak_var': 'mean', 'price_off_peak_fix': 'mean','price_peak_var' : 'mean','price_peak_fix' : 'mean','price_mid_peak_var' : 'mean','price_mid_peak_fix' : 'mean' }).reset_index()\n",
    "\n",
    "# Get january and december prices\n",
    "jan_prices = monthly_price_by_id.groupby('id').first().reset_index()\n",
    "dec_prices = monthly_price_by_id.groupby('id').last().reset_index()\n",
    "\n",
    "# Calculate the difference\n",
    "diff = pd.merge(dec_prices.rename(columns={'price_off_peak_var': 'dec_1_off_peak', 'price_off_peak_fix': 'dec_2_off_peak','price_peak_var' : 'dec_1_peak','price_peak_fix' : 'dec_2_peak','price_mid_peak_var' : 'dec_1_mid_peak', 'price_mid_peak_fix' : 'dec_2_mid_peak' }), jan_prices.drop(columns='price_date'), on='id')\n",
    "diff['offpeak_diff_dec_january_energy'] = diff['dec_1_off_peak'] - diff['price_off_peak_var']\n",
    "diff['offpeak_diff_dec_january_power'] = diff['dec_2_off_peak'] - diff['price_off_peak_fix']\n",
    "diff['peak_diff_dec_january_energy'] = diff['dec_1_peak'] - diff['price_peak_var']\n",
    "diff['peak_diff_dec_january_power'] = diff['dec_2_peak'] - diff['price_peak_fix']\n",
    "diff['midpeak_diff_dec_january_energy'] = diff['dec_1_mid_peak'] - diff['price_mid_peak_var']\n",
    "diff['midpeak_diff_dec_january_power'] = diff['dec_2_mid_peak'] - diff['price_mid_peak_fix']\n",
    "\n",
    "diff = diff[['id', 'offpeak_diff_dec_january_energy','offpeak_diff_dec_january_power','peak_diff_dec_january_energy','peak_diff_dec_january_power','midpeak_diff_dec_january_energy','midpeak_diff_dec_january_power']]\n",
    "diff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54b32a8-253b-45cd-9b44-780bc9b088c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(client_df, diff, on='id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e37a22-1701-4872-8de2-789504b93a5c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Period-to-Period Rate Differentials\n",
    "> **New Features**: Average Price Differences Between Time-of-Use Periods\n",
    "> \n",
    "> **Gas Rates**:\n",
    "> - Peak vs Off-Peak Difference\n",
    "> - Mid-Peak vs Peak Difference\n",
    "> - Off-Peak vs Mid-Peak Difference\n",
    ">\n",
    "> **Electricity Rates**:\n",
    "> - Peak vs Off-Peak Difference\n",
    "> - Mid-Peak vs Peak Difference\n",
    "> - Off-Peak vs Mid-Peak Difference\n",
    ">\n",
    "> **Hypothesis**: To analyze how price spreads between different time-of-use periods might influence customer churn behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681ecec4-22d5-4812-80bd-a1efb4189561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate average prices per period by company\n",
    "mean_prices = price_df.groupby(['id']).agg({\n",
    "    'price_off_peak_var': 'mean', \n",
    "    'price_peak_var': 'mean', \n",
    "    'price_mid_peak_var': 'mean',\n",
    "    'price_off_peak_fix': 'mean',\n",
    "    'price_peak_fix': 'mean',\n",
    "    'price_mid_peak_fix': 'mean'    \n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd92904-e350-4009-ac05-b29aa30cb7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean difference between consecutive periods\n",
    "mean_prices['peak_off_peak_var_mean_diff'] = mean_prices['price_peak_var'] - mean_prices['price_off_peak_var']\n",
    "mean_prices['mid_peak_peak_var_mean_diff'] = mean_prices['price_mid_peak_var'] - mean_prices['price_peak_var']\n",
    "mean_prices['off_peak_mid_peak_var_mean_diff'] = mean_prices['price_off_peak_var'] - mean_prices['price_mid_peak_var']\n",
    "mean_prices['peak_off_peak_fix_mean_diff'] = mean_prices['price_peak_fix'] - mean_prices['price_off_peak_fix']\n",
    "mean_prices['mid_peak_peak_fix_mean_diff'] = mean_prices['price_mid_peak_fix'] - mean_prices['price_peak_fix']\n",
    "mean_prices['off_peak_mid_peak_fix_mean_diff'] = mean_prices['price_off_peak_fix'] - mean_prices['price_mid_peak_fix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387d1313-c1ea-412e-a549-38de8a7a78fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'id', \n",
    "    'peak_off_peak_var_mean_diff',\n",
    "    'mid_peak_peak_var_mean_diff', \n",
    "    'off_peak_mid_peak_var_mean_diff',\n",
    "    'peak_off_peak_fix_mean_diff', \n",
    "    'mid_peak_peak_fix_mean_diff', \n",
    "    'off_peak_mid_peak_fix_mean_diff'\n",
    "]\n",
    "df = pd.merge(df, mean_prices[columns], on='id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abeb29f-919f-4b5a-b006-ae2b49440498",
   "metadata": {},
   "source": [
    "The dec-jan feature may reveal macro patterns that occur over an entire year, whereas inter-time-period features may reveal patterns on a micro scale between months."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2564dda8-81c2-41bc-a48f-4b325cd1e2a0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Maximum of Monthly Period-to-Period Rate Differentials\n",
    "> **New Features**: Maximum Price Differences Between Time-of-Use Periods\n",
    "> \n",
    "> **Gas Rates**:\n",
    "> - Peak vs Off-Peak Difference\n",
    "> - Mid-Peak vs Peak Difference\n",
    "> - Off-Peak vs Mid-Peak Difference\n",
    ">\n",
    "> **Electricity Rates**:\n",
    "> - Peak vs Off-Peak Difference\n",
    "> - Mid-Peak vs Peak Difference\n",
    "> - Off-Peak vs Mid-Peak Difference\n",
    ">\n",
    "> **Hypothesis**: To analyze how biggest price spreads between different time-of-use periods might influence customer churn behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836c67fd-303d-4681-8ed6-3c71184ad88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate average prices per period by company\n",
    "mean_prices_by_month = price_df.groupby(['id', 'price_date']).agg({\n",
    "    'price_off_peak_var': 'mean', \n",
    "    'price_peak_var': 'mean', \n",
    "    'price_mid_peak_var': 'mean',\n",
    "    'price_off_peak_fix': 'mean',\n",
    "    'price_peak_fix': 'mean',\n",
    "    'price_mid_peak_fix': 'mean'    \n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b905f65c-c1e0-422d-ae47-140f9cd74e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_prices_by_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba690d0a-79c3-4f9b-8132-80ec65da6689",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_prices_by_month['peak_off_peak_var_mean_diff'] = mean_prices_by_month['price_peak_var'] - mean_prices_by_month['price_off_peak_var']\n",
    "mean_prices_by_month['mid_peak_peak_var_mean_diff'] = mean_prices_by_month['price_mid_peak_var'] - mean_prices_by_month['price_peak_var']\n",
    "mean_prices_by_month['off_peak_mid_peak_var_mean_diff'] = mean_prices_by_month['price_off_peak_var'] - mean_prices_by_month['price_mid_peak_var']\n",
    "mean_prices_by_month['peak_off_peak_fix_mean_diff'] = mean_prices_by_month['price_peak_fix'] - mean_prices_by_month['price_off_peak_fix']\n",
    "mean_prices_by_month['mid_peak_peak_fix_mean_diff'] = mean_prices_by_month['price_mid_peak_fix'] - mean_prices_by_month['price_peak_fix']\n",
    "mean_prices_by_month['off_peak_mid_peak_fix_mean_diff'] = mean_prices_by_month['price_off_peak_fix'] - mean_prices_by_month['price_mid_peak_fix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ba18cf-f988-4b12-83da-c2bf6972fd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the maximum monthly difference across time periods\n",
    "max_diff_across_periods_months = mean_prices_by_month.groupby(['id']).agg({\n",
    "    'peak_off_peak_var_mean_diff': 'min',\n",
    "    'mid_peak_peak_var_mean_diff': 'min',\n",
    "    'off_peak_mid_peak_var_mean_diff': 'max',\n",
    "    'peak_off_peak_fix_mean_diff': 'min',\n",
    "    'mid_peak_peak_fix_mean_diff': 'min',\n",
    "    'off_peak_mid_peak_fix_mean_diff': 'max'\n",
    "}).reset_index().rename(\n",
    "    columns={\n",
    "        'peak_off_peak_var_mean_diff': 'peak_off_peak_var_max_monthly_diff',\n",
    "        'mid_peak_peak_var_mean_diff': 'mid_peak_peak_var_max_monthly_diff',\n",
    "        'off_peak_mid_peak_var_mean_diff': 'off_peak_mid_peak_var_max_monthly_diff',\n",
    "        'peak_off_peak_fix_mean_diff': 'peak_off_peak_fix_max_monthly_diff',\n",
    "        'mid_peak_peak_fix_mean_diff': 'mid_peak_peak_fix_max_monthly_diff',\n",
    "        'off_peak_mid_peak_fix_mean_diff': 'off_peak_mid_peak_fix_max_monthly_diff'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a2226d-0608-4155-907d-65672393f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'id',\n",
    "    'peak_off_peak_var_max_monthly_diff',\n",
    "    'mid_peak_peak_var_max_monthly_diff',\n",
    "    'off_peak_mid_peak_var_max_monthly_diff',\n",
    "    'peak_off_peak_fix_max_monthly_diff',\n",
    "    'mid_peak_peak_fix_max_monthly_diff',\n",
    "    'off_peak_mid_peak_fix_max_monthly_diff'\n",
    "]\n",
    "\n",
    "df = pd.merge(df, max_diff_across_periods_months[columns], on='id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3507a506-d7ab-499c-b2ec-8a9628fd4470",
   "metadata": {},
   "source": [
    "I thought that calculating the maximum price change between months and time periods would be a valuable feature to include, as I wanted to consider the perspective of a PowerCo client. For utility customers, sudden price fluctuations between months can be especially frustrating, and a significant increase within a short time frame could prompt them to explore other providers for better rates. Since our goal is to predict churn in this case, I believe this feature could offer meaningful insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aac497-8a39-4390-8e4e-86343a5c418e",
   "metadata": {},
   "source": [
    "---\n",
    "### Tenure\n",
    "\n",
    "How long a company has been a client of PowerCo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978911f4-98ad-4c10-9e9c-e1d6c900d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tenure'] = ((df['date_end'] - df['date_activ']) / np.timedelta64(1, 'D') / 365.25).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fe6101-6391-4f24-aa5f-976046553542",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['tenure']).agg({'churn': 'mean'}).sort_values(by='churn', ascending=False).plot(kind='bar',rot=360,figsize=(10,5))\n",
    "plt.title('Customer Churn Rate by Tenure Length (in Year)', weight = 'bold', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4f84dd-ea58-482c-bec2-c1569ee6e0ab",
   "metadata": {},
   "source": [
    "We can see that companies who have only been a client for 4 or less years are much more likely to churn compared to companies that have been a client for longer. Interestingly, the difference between 4 and 5 months is about 4%, which represents a large jump in likelihood for a customer to churn compared to the other differences between ordered tenure values. Perhaps this reveals that getting a customer to over 4 months tenure is actually a large milestone with respect to keeping them as a long term customer. \n",
    "\n",
    "This is an interesting feature to keep for modelling because clearly how long you've been a client, has a influence on the chance of a client churning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae03dfa-4922-4438-8052-c5d8d143f682",
   "metadata": {},
   "source": [
    "---\n",
    "### Transform `datetime` features into numeric values representing the number of months (as integers)\n",
    "\n",
    "- months_activ = Number of months active until reference date (Jan 2016)\n",
    "- months_to_end = Number of months of the contract left until reference date (Jan 2016)\n",
    "- months_modif_prod = Number of months since last modification until reference date (Jan 2016)\n",
    "- months_renewal = Number of months since last renewal until reference date (Jan 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8b9862-7b23-461d-937f-2e9dc732b4a4",
   "metadata": {},
   "source": [
    "Dates as a datetime object are not useful for a predictive model, so we needed to use the datetimes to create some other features that may hold some predictive power. \n",
    "\n",
    "Using intuition, you could assume that a client who has been an active client of PowerCo for a longer amount of time may have more loyalty to the brand and is more likely to stay. Whereas a newer client may be more volatile. Hence the addition of the `months_activ` feature.\n",
    "\n",
    "As well as this, if we think from the perspective of a client with PowerCo, if you're coming toward the end of your contract with PowerCo your thoughts could go a few ways. You could be looking for better deals for when your contract ends, or you might want to see out your contract and sign another one. One the other hand if you've only just joined, you may have a period where you're allowed to leave if you're not satisfied. Furthermore, if you're in the middle of your contract, their may be charges if you wanted to leave, deterring clients from churning mid-way through their agreement. So, I think `months_to_end` will be an interesting feature because it may reveal patterns and behaviours about timing of churn.\n",
    "\n",
    "My belief is that if a client has made recent updates to their contract, they are more likely to be satisfied or at least they have received a level of customer service to update or change their existing services. I believe this to be a positive sign, they are an engaged customer, and so I believe `months_modif_prod` will be an interesting feature to include because it shows the degree of how 'engaged' a client is with PowerCo.\n",
    "\n",
    "Finally the number of months since a client last renewed a contract I believe will be an interesting feature because once again, it shows the degree to which that client is engaged. It also goes a step further than just engagement, it shows a level of commitment if a client renews their contract. For this reason, I believe `months_renewal` will be a good feature to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fe56bb-f358-4e48-8269-dca6547dc277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_months(reference_date, df, column):\n",
    "    \"\"\"\n",
    "    Input a column with timedeltas and return months\n",
    "    \"\"\"\n",
    "    time_delta = reference_date - df[column]\n",
    "    months = (time_delta / np.timedelta64(1, 'D') / 30.44).astype(int)\n",
    "    return months\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845bd570-96d0-45ba-ac80-b1f723d39835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reference date\n",
    "reference_date = datetime(2016, 1, 1)\n",
    "\n",
    "# Create columns\n",
    "df['months_activ'] = convert_months(reference_date, df, 'date_activ')\n",
    "df['months_to_end'] = -convert_months(reference_date, df, 'date_end')\n",
    "df['months_modif_prod'] = convert_months(reference_date, df, 'date_modif_prod')\n",
    "df['months_renewal'] = convert_months(reference_date, df, 'date_renewal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78b26b3-1eda-4fe5-9d75-1a0a6fdecfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We no longer need the datetime columns that we used for feature engineering, so we can drop them\n",
    "remove = [\n",
    "    'date_activ',\n",
    "    'date_end',\n",
    "    'date_modif_prod',\n",
    "    'date_renewal'\n",
    "]\n",
    "\n",
    "df = df.drop(columns=remove)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676a0d2c-2a6b-4f87-8a65-e8735b740e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['months_activ','months_to_end','months_modif_prod','months_renewal']\n",
    "x_tick_rot_list = [45,360,45,360]\n",
    "x_tick_font_size_list = [10,12,10,12]\n",
    "\n",
    "for col,x_tick_rot,x_tick_font_size in zip(columns,x_tick_rot_list,x_tick_font_size_list) : \n",
    "    df.groupby([col]).agg({'churn': 'mean'}).sort_values(by='churn', ascending=False).plot(kind='bar',rot=x_tick_rot, figsize = (25,8))\n",
    "    plt.title(f'Customer Churn Rate by {col} ', weight = 'bold', fontsize = 20)\n",
    "    plt.xticks(fontsize=x_tick_font_size)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40fc54b-c2b7-41f2-a569-54e1c26e3b23",
   "metadata": {},
   "source": [
    "there might be association between months_activ/months_modif_prod/months_renewal and churn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd37cd4-c000-4033-9e83-e99b71f8088c",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Data Preprocessing\n",
    "### Encoding Categorical Data for Predictive Modeling\n",
    "\n",
    "A predictive model cannot accept categorical or `string` values, hence as a data scientist you need to encode categorical features into numerical representations in the most compact and discriminative way possible.\n",
    "\n",
    "The simplest method is to map each category to an integer (label encoding), however this is not always appropriate beecause it then introduces the concept of an order into a feature which may not inherently be present `0 < 1 < 2 < 3 ...`\n",
    "\n",
    "Another way to encode categorical features is to use `dummy variables` AKA `one hot encoding`. This create a new feature for every unique value of a categorical column, and fills this column with either a 1 or a 0 to indicate that this company does or does not belong to this category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0c0ddd-0d91-437e-89aa-a758efa3792f",
   "metadata": {},
   "source": [
    "#### has_gas\n",
    "\n",
    "Transform this column from being categorical to being a binary flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e0a94a-c1a8-440e-ad4b-da501e1815b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['has_gas'] = df['has_gas'].replace(['t', 'f'], [1, 0])\n",
    "df.groupby(['has_gas']).agg({'churn': 'mean'}).plot(kind='bar',rot=360, figsize=(10,8))\n",
    "plt.title('Customer Churn Rate by Gas Service Subscription Status', weight = 'bold', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed473f4a-2b92-4edc-a984-d5b9960f674a",
   "metadata": {},
   "source": [
    "If a customer also buys gas from PowerCo, it shows that they have multiple products and are a loyal customer to the brand. Hence, it is no surprise that customers who do not buy gas are almost 2% more likely to churn than customers who also buy gas from PowerCo. Hence, this is a useful feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b14fb28-fda0-4bf6-b24c-468751c79412",
   "metadata": {},
   "source": [
    "#### channel_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349f4f1c-e272-4232-8c6a-d5fab3e71965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform into categorical type\n",
    "df['channel_sales'] = df['channel_sales'].astype('category')\n",
    "\n",
    "# Let's see how many categories are within this column\n",
    "df['channel_sales'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb455562-078d-4203-b03c-3cab25f27888",
   "metadata": {},
   "source": [
    "We have 8 categories, so we will create 8 dummy variables from this column. However, as you can see the last 3 categories in the output above, show that they only have 11, 3 and 2 occurrences respectively. Considering that our dataset has about 14000 rows, this means that these dummy variables will be almost entirely 0 and so will not add much predictive power to the model at all (since they're almost entirely a constant value and provide very little).\n",
    "\n",
    "For this reason, we will drop these 3 dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454a8155-ac2d-4ad4-a4d5-93e47e3688a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['channel_sales'], prefix='channel')\n",
    "df = df.drop(columns=['channel_sddiedcslfslkckwlfkdpoeeailfpeds', 'channel_epumfxlbckeskwekxbiuasklxalciiuu', 'channel_fixdbufsefwooaasfcxdxadsiekoceaa'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c16f0d-6ce8-4ff5-9207-c420c44c8aaa",
   "metadata": {},
   "source": [
    "#### origin_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936b3774-3d0e-4a25-a6b8-3eef563b409b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform into categorical type\n",
    "df['origin_up'] = df['origin_up'].astype('category')\n",
    "\n",
    "# Let's see how many categories are within this column\n",
    "df['origin_up'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a3b3a6-8999-4502-a28a-0ced53ee7e89",
   "metadata": {},
   "source": [
    "Similar to `channel_sales` the last 3 categories in the output above show very low frequency, so we will remove these from the features after creating dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541717a5-974f-469b-ba84-6bfb9d462f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['origin_up'], prefix='origin_up')\n",
    "df = df.drop(columns=['origin_up_MISSING', 'origin_up_usapbepcfoloekilkwsdiboslwaxobdp', 'origin_up_ewxeelcelemmiwuafmddpobolfuxioce'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b242d27-6fc3-4c91-a7d5-3ddd157fefc6",
   "metadata": {},
   "source": [
    "---\n",
    "### Addressing Skewness in Numerical Data\n",
    "\n",
    "In the previous exercise we saw that some variables were highly skewed. The reason why we need to treat skewness is because some predictive models have inherent assumptions about the distribution of the features that are being supplied to it. Such models are called `parametric` models, and they typically assume that all variables are both independent and normally distributed. \n",
    "\n",
    "Skewness isn't always a bad thing, but as a rule of thumb it is always good practice to treat highly skewed variables because of the reason stated above, but also as it can improve the speed at which predictive models are able to converge to its best solution.\n",
    "\n",
    "There are many ways that you can treat skewed variables. You can apply transformations such as:\n",
    "- Square root\n",
    "- Cubic root\n",
    "- Logarithm\n",
    "\n",
    "to a continuous numeric column and you will notice the distribution changes. For this use case we will use the 'Logarithm' transformation for the positively skewed features. \n",
    "\n",
    "<b>Note:</b> We cannot apply log to a value of 0, so we will add a constant of 1 to all the values\n",
    "\n",
    "First I want to see the statistics of the skewed features, so that we can compare before and after transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed15af3-e657-4b5b-82b3-a77f7e9075ca",
   "metadata": {},
   "source": [
    "#### original columns in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a653bb-6c82-4908-9d17-82a5a20709e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed = [\n",
    "    'cons_12m', \n",
    "    'cons_gas_12m', \n",
    "    'cons_last_month',\n",
    "    'forecast_cons_12m', \n",
    "    'forecast_cons_year', \n",
    "    'forecast_discount_energy',\n",
    "    'forecast_meter_rent_12m', \n",
    "    'forecast_price_energy_off_peak',\n",
    "    'forecast_price_energy_peak', \n",
    "    'forecast_price_pow_off_peak'\n",
    "]\n",
    "\n",
    "# Calculate skewness for each column in the skewed list\n",
    "skewness_values = df[skewed].apply(lambda x: skew(x.dropna()))\n",
    "\n",
    "# Display skewness values\n",
    "skewness_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eccb48-9b4a-4013-8554-7cc6e20d038f",
   "metadata": {},
   "source": [
    "We can see that the skewness for most of these features is quite high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0557ed9-2463-4732-8e61-f7a61068fbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log10 transformation\n",
    "df[\"cons_12m\"] = np.log10(df[\"cons_12m\"] + 1)\n",
    "df[\"cons_gas_12m\"] = np.log10(df[\"cons_gas_12m\"] + 1)\n",
    "df[\"cons_last_month\"] = np.log10(df[\"cons_last_month\"] + 1)\n",
    "df[\"forecast_cons_12m\"] = np.log10(df[\"forecast_cons_12m\"] + 1)\n",
    "df[\"forecast_cons_year\"] = np.log10(df[\"forecast_cons_year\"] + 1)\n",
    "df[\"forecast_meter_rent_12m\"] = np.log10(df[\"forecast_meter_rent_12m\"] + 1)\n",
    "df[\"imp_cons\"] = np.log10(df[\"imp_cons\"] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacd1a1d-482f-4b01-a01e-cfb1ca645c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate skewness for each column in the skewed list\n",
    "skewness_values = df[skewed].apply(lambda x: skew(x.dropna()))\n",
    "\n",
    "# Display skewness values\n",
    "skewness_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74818a92-f70d-4bd9-9490-79802b1306e4",
   "metadata": {},
   "source": [
    "Now we can see that for the majority of the features, their skewness is much lower after transformation. This is a good thing, it shows that these features are more stable and predictable now.\n",
    "\n",
    "Let's quickly check the distributions of some of these features too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fab9302-ce0c-4b4c-9ff9-41be60df1931",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, figsize=(18, 20))\n",
    "\n",
    "\n",
    "# Plot histograms\n",
    "sns.distplot((df[\"cons_12m\"].dropna()), ax=axs[0])\n",
    "sns.distplot((df[df[\"has_gas\"]==1][\"cons_gas_12m\"].dropna()), ax=axs[1])\n",
    "sns.distplot((df[\"cons_last_month\"].dropna()), ax=axs[2])\n",
    "\n",
    "axs[0].set_title(\"Log-Transformed Distribution of cons_12m\", fontsize=20, weight = 'bold', pad=15)\n",
    "axs[1].set_title(\"Log-Transformed Distribution of cons_gas_12m\", fontsize=20, weight = 'bold', pad=15)\n",
    "axs[2].set_title(\"Log-Transformed Distribution of cons_last_month\", fontsize=20, weight = 'bold', pad=15)\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9370dda9-cc31-4933-ad7a-7d563a4f802f",
   "metadata": {},
   "source": [
    "#### columns derived from feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6425cfda-5916-4a8c-bcc5-90eb2bd7cb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [ 'offpeak_diff_dec_january_energy', 'offpeak_diff_dec_january_power',\n",
    "       'peak_diff_dec_january_energy', 'peak_diff_dec_january_power',\n",
    "       'midpeak_diff_dec_january_energy', 'midpeak_diff_dec_january_power',\n",
    "       'peak_off_peak_var_mean_diff', 'mid_peak_peak_var_mean_diff',\n",
    "       'off_peak_mid_peak_var_mean_diff', 'peak_off_peak_fix_mean_diff',\n",
    "       'mid_peak_peak_fix_mean_diff', 'off_peak_mid_peak_fix_mean_diff',\n",
    "       'peak_off_peak_var_max_monthly_diff',\n",
    "       'mid_peak_peak_var_max_monthly_diff',\n",
    "       'off_peak_mid_peak_var_max_monthly_diff',\n",
    "       'peak_off_peak_fix_max_monthly_diff',\n",
    "       'mid_peak_peak_fix_max_monthly_diff',\n",
    "       'off_peak_mid_peak_fix_max_monthly_diff']\n",
    "\n",
    "# Calculate skewness for each column in the skewed list\n",
    "skewness_values = df[columns].apply(lambda x: skew(x.dropna()))\n",
    "\n",
    "# Display skewness values\n",
    "skewness_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce20c46-2b95-40b0-b093-7a0b83ea29a1",
   "metadata": {},
   "source": [
    "Since the skewness values for most of these columns are not extreme, so only apply transformations to 'offpeak_diff_dec_january_power'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409ca078-c389-4614-be12-a1861b3223dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"offpeak_diff_dec_january_power\"] = np.log10(df[\"offpeak_diff_dec_january_power\"] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a92c767-2f16-414f-8844-491aae50d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness_values = df[['offpeak_diff_dec_january_power']].apply(lambda x: skew(x.dropna()))\n",
    "skewness_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1072b8-c1dd-4ced-ae56-18217e1b48ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, figsize=(18, 10))\n",
    "sns.distplot((df['offpeak_diff_dec_january_power'].dropna()), ax=axs)\n",
    "axs.set_title(\"Log-Transformed Distribution of offpeak_diff_dec_january_power\", fontsize=20, weight = 'bold', pad=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c5c223-257c-4780-94ce-49bd2ae9de2e",
   "metadata": {},
   "source": [
    "---\n",
    "### Correlation Analysis\n",
    "\n",
    "Another thing that is always useful to look at is how correlated all of the features are within your dataset.\n",
    "\n",
    "One of the assumptions of any parametric predictive model (as stated earlier) is that all features must be independent.\n",
    "\n",
    "For features to be independent, this means that each feature must have absolutely no dependence on any other feature. If two features are highly correlated and share similar information, this breaks this assumption. \n",
    "\n",
    "Ideally, you want a set of features that have 0 correlation with all of the independent variables (all features except our target variable) and a high correlation with the target variable (churn). However, this is very rarely the case and it is common to have a small degree of correlation between independent features.\n",
    "\n",
    "So now let's look at how all the features within the model are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c40dd0-3b1b-4712-af03-bb66d28efd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['cons_12m', 'cons_gas_12m', 'cons_last_month',\n",
    "       'forecast_cons_12m', 'forecast_cons_year', 'forecast_discount_energy',\n",
    "       'forecast_meter_rent_12m', 'forecast_price_energy_off_peak',\n",
    "       'forecast_price_energy_peak', 'forecast_price_pow_off_peak', 'has_gas',\n",
    "       'imp_cons', 'margin_gross_pow_ele', 'margin_net_pow_ele', 'nb_prod_act',\n",
    "       'net_margin', 'num_years_antig', 'pow_max', 'churn',\n",
    "       'offpeak_diff_dec_january_energy', 'offpeak_diff_dec_january_power',\n",
    "       'peak_diff_dec_january_energy', 'peak_diff_dec_january_power',\n",
    "       'midpeak_diff_dec_january_energy', 'midpeak_diff_dec_january_power',\n",
    "       'peak_off_peak_var_mean_diff', 'mid_peak_peak_var_mean_diff',\n",
    "       'off_peak_mid_peak_var_mean_diff', 'peak_off_peak_fix_mean_diff',\n",
    "       'mid_peak_peak_fix_mean_diff', 'off_peak_mid_peak_fix_mean_diff',\n",
    "       'peak_off_peak_var_max_monthly_diff',\n",
    "       'mid_peak_peak_var_max_monthly_diff',\n",
    "       'off_peak_mid_peak_var_max_monthly_diff',\n",
    "       'peak_off_peak_fix_max_monthly_diff',\n",
    "       'mid_peak_peak_fix_max_monthly_diff',\n",
    "       'off_peak_mid_peak_fix_max_monthly_diff', 'tenure', 'months_activ',\n",
    "       'months_to_end', 'months_modif_prod', 'months_renewal']\n",
    "\n",
    "correlation_matrix = df[columns].corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83566a7e-1456-49e8-be60-afe0e7221748",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(70,70))\n",
    "sns.heatmap(correlation_matrix,cmap=\"YlGnBu\",annot_kws={'size': 27}, annot=True)\n",
    "plt.title('Correlation Matrix of all variables', weight = 'bold', fontsize = 60)\n",
    "plt.xticks(fontsize=30,rotation=45)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2de8da1-9f6f-4d24-afb6-296d457ebb06",
   "metadata": {},
   "source": [
    "Since there are too many columns in dataframe, the heatmap is a bit messy. Therefore, I'll filter out pairs of variables which have high correlation manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4fbdc0-d085-4aec-b2bb-a51f662be046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find pairs with correlation > 0.9 (excluding self-correlation of 1.0)\n",
    "high_correlation_pairs = (\n",
    "    correlation_matrix\n",
    "    .where((abs(correlation_matrix) > 0.9) & (correlation_matrix != 1.0))\n",
    "    .stack()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Rename columns for better readability\n",
    "high_correlation_pairs.columns = ['Variable 1', 'Variable 2', 'Correlation']\n",
    "\n",
    "high_correlation_pairs['Sorted Pair'] = high_correlation_pairs.apply(\n",
    "    lambda row: tuple(sorted([row['Variable 1'], row['Variable 2']])), axis=1\n",
    ")\n",
    "\n",
    "unique_high_correlation_pairs = high_correlation_pairs.drop_duplicates(subset=['Sorted Pair']).drop(columns=['Sorted Pair'])\n",
    "# Display high-correlation pairs\n",
    "unique_high_correlation_pairs = unique_high_correlation_pairs.sort_values(by=['Variable 1','Variable 2']).reset_index(drop=True)\n",
    "unique_high_correlation_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ada15ff-023c-48e7-b10e-704463c9afbe",
   "metadata": {},
   "source": [
    "#### Suggestions for Dropping Columns\n",
    "There are lots of variables pairs with high correlation. Therefore, we would want to drop some variables to ensure the independence between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef18c6a4-f9ca-487c-a915-f8e5957825e9",
   "metadata": {},
   "source": [
    "drop \n",
    "- has_gas\n",
    "- forecast_cons_year\n",
    "- forecast_price_energy_peak\n",
    "- margin_gross_pow_ele\n",
    "- mid_peak_peak_fix_mean_diff\n",
    "- midpeak_diff_dec_january_energy\n",
    "- num_years_antig\n",
    "- months_activ\n",
    "- off_peak_mid_peak_fix_mean_diff\n",
    "- off_peak_mid_peak_var_max_monthly_diff\n",
    "- off_peak_mid_peak_var_mean_diff\n",
    "- peak_diff_dec_january_power\n",
    "- peak_off_peak_fix_max_monthly_diff\n",
    "- peak_off_peak_fix_mean_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fd98af-36e1-4b17-ae4d-1f34562edb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    'has_gas',\n",
    "    'forecast_cons_year',\n",
    "    'forecast_price_energy_peak',\n",
    "    'margin_gross_pow_ele',\n",
    "    'mid_peak_peak_fix_mean_diff',\n",
    "    'midpeak_diff_dec_january_energy',\n",
    "    'num_years_antig',\n",
    "    'months_activ',\n",
    "    'off_peak_mid_peak_fix_mean_diff',\n",
    "    'off_peak_mid_peak_var_max_monthly_diff',\n",
    "    'off_peak_mid_peak_var_mean_diff',\n",
    "    'peak_diff_dec_january_power',\n",
    "    'peak_off_peak_fix_max_monthly_diff',\n",
    "    'peak_off_peak_fix_mean_diff'\n",
    "]\n",
    "\n",
    "df = df.drop(columns_to_drop, axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e345eae-9d6d-4431-9a4c-3daae0103a37",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Export the data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35fb33c-3480-4466-ac46-234c197a4c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('data_for_modeling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580640b7-4444-4fa1-bc31-fbe55495c82e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-3-with-sklearn",
   "language": "python",
   "name": "python-3-with-sklearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
